"""Streamlitãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³"""

import os
import time
import streamlit as st
from dotenv import load_dotenv
from typing import Dict, List

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

from constants import (
    DEFAULT_DATA_FOLDER, DEFAULT_K, DEFAULT_BM25_WEIGHT, DEFAULT_VECTOR_WEIGHT,
    DEFAULT_LLM_MODEL, MAX_CONVERSATION_HISTORY
)
from initialize import initialize_system
from retriever import search_with_scores, update_retriever_weights, update_retriever_k
from guardrails import apply_guardrails
from logger import log_query
from components import (
    render_citation, render_danger_banner, render_security_notice,
    render_chat_message
)

# ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
load_dotenv()

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="AIã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£é‹ç”¨æ‰‹é †æ›¸ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ",
    page_icon="ğŸ”’",
    layout="wide"
)

# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
if 'messages' not in st.session_state:
    st.session_state.messages = []

if 'vectorstore' not in st.session_state:
    st.session_state.vectorstore = None

if 'hybrid_retriever' not in st.session_state:
    st.session_state.hybrid_retriever = None

if 'chunks_metadata' not in st.session_state:
    st.session_state.chunks_metadata = {}

if 'data_folder' not in st.session_state:
    st.session_state.data_folder = DEFAULT_DATA_FOLDER

if 'k' not in st.session_state:
    st.session_state.k = DEFAULT_K

if 'bm25_weight' not in st.session_state:
    st.session_state.bm25_weight = DEFAULT_BM25_WEIGHT

if 'vector_weight' not in st.session_state:
    st.session_state.vector_weight = DEFAULT_VECTOR_WEIGHT

if 'index_last_built' not in st.session_state:
    st.session_state.index_last_built = None

if 'index_count' not in st.session_state:
    st.session_state.index_count = 0

if 'session_id' not in st.session_state:
    import uuid
    st.session_state.session_id = str(uuid.uuid4())


def handle_query(user_query: str, session_state: Dict) -> Dict:
    """è³ªå•å‡¦ç†ã®ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
    start_time = time.time()

    # Retrieverã®é‡ã¿ã¨kã‚’æ›´æ–°
    if session_state['hybrid_retriever'] is not None:
        update_retriever_weights(
            session_state['hybrid_retriever'],
            session_state['bm25_weight'],
            session_state['vector_weight']
        )
        update_retriever_k(
            session_state['hybrid_retriever'],
            session_state['k']
        )

    # æ¤œç´¢å®Ÿè¡Œ
    search_results = search_with_scores(
        ensemble_retriever=session_state['hybrid_retriever'],
        query=user_query,
        k=session_state['k']
    )
    
    # ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«é©ç”¨ï¼ˆæ¤œç´¢å¾Œã€LLMå‘¼ã³å‡ºã—å‰ï¼‰
    guardrail_result = apply_guardrails(
        query=user_query,
        search_results=search_results,
        answer=None  # ã¾ã å›ç­”ã¯ç”Ÿæˆã—ã¦ã„ãªã„
    )
    
    # æ ¹æ‹ ä¸è¶³ã¾ãŸã¯æ›–æ˜§è³ªå•ã®å ´åˆã¯ã€ã“ã“ã§çµ‚äº†
    if not guardrail_result['should_respond']:
        processing_time = time.time() - start_time
        
        return {
            'answer': guardrail_result['answer'],
            'citations': guardrail_result['citations'],
            'flags': guardrail_result['flags'],
            'warning_reason': guardrail_result['warning_reason'],
            'top_score': guardrail_result['top_score'],
            'processing_time': processing_time,
            'token_usage': {'prompt_tokens': 0, 'completion_tokens': 0, 'total_tokens': 0},
            'cost': 0.0
        }
    
    # LLMãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
    context_text = "\n\n".join([
        f"ã€{i+1}ã€‘{result['text']}\nï¼ˆå‡ºå…¸: {result['file']} > {result['heading']}ï¼‰"
        for i, result in enumerate(search_results[:session_state['k']])
    ])
    
    # ä¼šè©±å±¥æ­´ã‚’å–å¾—ï¼ˆæœ€æ–°5å¾€å¾©ï¼‰
    conversation_history = session_state['messages'][-MAX_CONVERSATION_HISTORY * 2:]
    history_text = ""
    for msg in conversation_history:
        if msg['role'] == 'user':
            history_text += f"ãƒ¦ãƒ¼ã‚¶ãƒ¼: {msg['content']}\n"
        elif msg['role'] == 'assistant':
            history_text += f"ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ: {msg['content']}\n"
    
    prompt_template = ChatPromptTemplate.from_messages([
        ("system", """ã‚ãªãŸã¯SOCï¼ˆã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£é‹ç”¨ã‚»ãƒ³ã‚¿ãƒ¼ï¼‰ã®é‹ç”¨æ‰‹é †æ›¸ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
æä¾›ã•ã‚ŒãŸæ‰‹é †æ›¸ã®å†…å®¹ã«åŸºã¥ã„ã¦ã€æ­£ç¢ºã§åˆ†ã‹ã‚Šã‚„ã™ã„å›ç­”ã‚’ã—ã¦ãã ã•ã„ã€‚

é‡è¦ãªãƒ«ãƒ¼ãƒ«:
- æ‰‹é †æ›¸ã®å†…å®¹ã®ã¿ã‚’æ ¹æ‹ ã¨ã—ã¦å›ç­”ã—ã¦ãã ã•ã„
- æ¨æ¸¬ã‚„æ†¶æ¸¬ã¯é¿ã‘ã€æ‰‹é †æ›¸ã«è¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹æƒ…å ±ã®ã¿ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„
- å›ç­”ã®æœ€å¾Œã«ã€å‚è€ƒã«ã—ãŸæ‰‹é †æ›¸ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ˜è¨˜ã—ã¦ãã ã•ã„
- åˆ†ã‹ã‚‰ãªã„å ´åˆã¯ã€Œè©²å½“ã™ã‚‹æ‰‹é †ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€ã¨å›ç­”ã—ã¦ãã ã•ã„"""),
        ("human", """ä»¥ä¸‹ã®æ‰‹é †æ›¸ã®å†…å®¹ã‚’å‚è€ƒã«ã€è³ªå•ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚

ã€æ‰‹é †æ›¸ã®å†…å®¹ã€‘
{context}

ã€ä¼šè©±å±¥æ­´ã€‘
{history}

ã€è³ªå•ã€‘
{question}

ã€å›ç­”ã€‘""")
    ])
    
    # LLMåˆæœŸåŒ–
    llm = ChatOpenAI(
        model=DEFAULT_LLM_MODEL,
        temperature=0.0
    )
    
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå®Ÿè¡Œ
    prompt = prompt_template.format_messages(
        context=context_text,
        history=history_text,
        question=user_query
    )
    
    response = llm.invoke(prompt)
    answer = response.content.strip()
    
    # ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«é©ç”¨ï¼ˆå›ç­”ç”Ÿæˆå¾Œã€å±é™ºæ“ä½œæ¤œçŸ¥ï¼‰
    guardrail_result = apply_guardrails(
        query=user_query,
        search_results=search_results,
        answer=answer  # ç”Ÿæˆã—ãŸå›ç­”ã‚’ãƒã‚§ãƒƒã‚¯
    )
    
    # ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã¨ã‚³ã‚¹ãƒˆè¨ˆç®—
    from utils import calculate_cost
    
    # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã‚’å–å¾—
    if hasattr(response, 'response_metadata') and 'token_usage' in response.response_metadata:
        token_usage = response.response_metadata['token_usage']
    else:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æ¦‚ç®—
        estimated_prompt_tokens = len(context_text.split()) + len(user_query.split()) * 2
        estimated_completion_tokens = len(answer.split()) * 1.3
        token_usage = {
            'prompt_tokens': int(estimated_prompt_tokens),
            'completion_tokens': int(estimated_completion_tokens),
            'total_tokens': int(estimated_prompt_tokens + estimated_completion_tokens)
        }
    
    cost = calculate_cost(
        token_usage.get('prompt_tokens', 0),
        token_usage.get('completion_tokens', 0),
        DEFAULT_LLM_MODEL
    )
    
    processing_time = time.time() - start_time
    
    return {
        'answer': guardrail_result['answer'],
        'citations': guardrail_result['citations'],
        'flags': guardrail_result['flags'],
        'warning_reason': guardrail_result['warning_reason'],
        'top_score': guardrail_result['top_score'],
        'processing_time': processing_time,
        'token_usage': token_usage,
        'cost': cost
    }


def render_sidebar():
    """ã‚µã‚¤ãƒ‰ãƒãƒ¼æç”»"""
    with st.sidebar:
        st.title("âš™ï¸ è¨­å®š")
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹
        data_folder = st.text_input(
            "ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹",
            value=st.session_state.data_folder,
            help="æ‰‹é †æ›¸ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆPDF/Markdownï¼‰ã‚’æ ¼ç´ã—ã¦ã„ã‚‹ãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ‘ã‚¹"
        )
        st.session_state.data_folder = data_folder
        
        # kè¨­å®š
        k = st.number_input(
            "æ¤œç´¢çµæœæ•° (k)",
            min_value=1,
            max_value=20,
            value=st.session_state.k,
            help="æ¤œç´¢çµæœã¨ã—ã¦å–å¾—ã™ã‚‹ãƒãƒ£ãƒ³ã‚¯æ•°"
        )
        st.session_state.k = int(k)
        
        # é‡ã¿è¨­å®š
        st.subheader("æ¤œç´¢é‡ã¿")
        bm25_weight = st.slider(
            "BM25é‡ã¿",
            min_value=0.0,
            max_value=1.0,
            value=st.session_state.bm25_weight,
            step=0.1,
            help="BM25æ¤œç´¢ã®é‡ã¿ï¼ˆ0.0-1.0ï¼‰"
        )
        st.session_state.bm25_weight = bm25_weight
        st.session_state.vector_weight = 1.0 - bm25_weight
        
        st.write(f"ãƒ™ã‚¯ãƒˆãƒ«é‡ã¿: {st.session_state.vector_weight:.1f}")
        
        st.divider()
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å†æ§‹ç¯‰ãƒœã‚¿ãƒ³
        if st.button("ğŸ”„ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å†æ§‹ç¯‰", type="primary"):
            with st.spinner("ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ§‹ç¯‰ä¸­..."):
                result = initialize_system(
                    st.session_state.data_folder,
                    bm25_weight=st.session_state.bm25_weight,
                    vector_weight=st.session_state.vector_weight,
                    k=st.session_state.k
                )
                st.session_state.vectorstore = result['vectorstore']
                st.session_state.hybrid_retriever = result['hybrid_retriever']
                st.session_state.chunks_metadata = result['chunks_metadata']
                st.session_state.index_count = result['index_count']
                st.session_state.index_last_built = result['index_last_built']
                st.success(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰å®Œäº†: {result['index_count']}ä»¶")
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹çŠ¶æ…‹è¡¨ç¤º
        st.divider()
        st.subheader("ğŸ“Š ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹çŠ¶æ…‹")
        if st.session_state.index_count > 0:
            st.write(f"**ãƒãƒ£ãƒ³ã‚¯æ•°**: {st.session_state.index_count}")
            if st.session_state.index_last_built:
                st.write(f"**æœ€çµ‚æ›´æ–°**: {st.session_state.index_last_built}")
        else:
            st.info("ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒæ§‹ç¯‰ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚å†æ§‹ç¯‰ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    
    # æ©Ÿå¯†æƒ…å ±æ³¨æ„è¡¨ç¤º
    render_security_notice()
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼
    render_sidebar()
    
    # ã‚¿ã‚¤ãƒˆãƒ«
    st.title("ğŸ”’ AIã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£é‹ç”¨æ‰‹é †æ›¸ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ")
    st.markdown("æ‰‹é †æ›¸ã‚’æ¤œç´¢ã—ã¦ã€è³ªå•ã«å›ç­”ã—ã¾ã™ã€‚")
    
    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒæ§‹ç¯‰ã•ã‚Œã¦ã„ãªã„å ´åˆã¯åˆæœŸåŒ–ã‚’ä¿ƒã™
    if st.session_state.hybrid_retriever is None:
        st.warning("âš ï¸ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒæ§‹ç¯‰ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰ã€Œã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å†æ§‹ç¯‰ã€ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")

        # è‡ªå‹•åˆæœŸåŒ–ã‚’è©¦ã¿ã‚‹
        if st.button("è‡ªå‹•åˆæœŸåŒ–ã‚’è©¦ã™"):
            with st.spinner("åˆæœŸåŒ–ä¸­..."):
                result = initialize_system(
                    st.session_state.data_folder,
                    bm25_weight=st.session_state.bm25_weight,
                    vector_weight=st.session_state.vector_weight,
                    k=st.session_state.k
                )
                if result['index_count'] > 0:
                    st.session_state.vectorstore = result['vectorstore']
                    st.session_state.hybrid_retriever = result['hybrid_retriever']
                    st.session_state.chunks_metadata = result['chunks_metadata']
                    st.session_state.index_count = result['index_count']
                    st.session_state.index_last_built = result['index_last_built']
                    st.success(f"åˆæœŸåŒ–å®Œäº†: {result['index_count']}ä»¶ã®ãƒãƒ£ãƒ³ã‚¯ã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–ã—ã¾ã—ãŸ")
                    st.rerun()
                else:
                    st.error("ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚©ãƒ«ãƒ€ã«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return
    
    # ä¼šè©±å±¥æ­´è¡¨ç¤º
    for message in st.session_state.messages:
        render_chat_message(
            role=message['role'],
            content=message['content'],
            citations=message.get('citations', [])
        )
        
        # å±é™ºæ“ä½œè­¦å‘ŠãƒãƒŠãƒ¼ï¼ˆå„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã”ã¨ã«è¡¨ç¤ºï¼‰
        if message.get('flags', {}).get('dangerous_operation', False):
            render_danger_banner()
    
    # è³ªå•å…¥åŠ›
    if prompt := st.chat_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„"):
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
        st.session_state.messages.append({
            'role': 'user',
            'content': prompt,
            'citations': []
        })
        
        # è³ªå•å‡¦ç†
        with st.spinner("å›ç­”ã‚’ç”Ÿæˆä¸­..."):
            result = handle_query(prompt, st.session_state)
            
            # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ ï¼ˆè­¦å‘Šã¯æ—¢ã«apply_guardrailsã§è¿½åŠ æ¸ˆã¿ï¼‰
            st.session_state.messages.append({
                'role': 'assistant',
                'content': result['answer'],
                'citations': result['citations'],
                'flags': result['flags'],
                'warning_reason': result.get('warning_reason')
            })
            
            # ãƒ­ã‚°è¨˜éŒ²
            log_query(
                query=prompt,
                search_results=result['citations'],
                answer=result['answer'],
                processing_time=result['processing_time'],
                token_usage=result['token_usage'],
                cost=result['cost'],
                search_config={
                    'k': st.session_state.k,
                    'bm25_weight': st.session_state.bm25_weight,
                    'vector_weight': st.session_state.vector_weight
                },
                flags=result['flags'],
                warning_reason=result.get('warning_reason'),
                top_score=result.get('top_score', 0.0),
                session_id=st.session_state.session_id
            )
        
        # ç”»é¢ã‚’æ›´æ–°
        st.rerun()


if __name__ == "__main__":
    # OpenAI APIã‚­ãƒ¼ã®ç¢ºèª
    if not os.getenv("OPENAI_API_KEY"):
        st.error("âš ï¸ OPENAI_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚.envãƒ•ã‚¡ã‚¤ãƒ«ã¾ãŸã¯ç’°å¢ƒå¤‰æ•°ã§è¨­å®šã—ã¦ãã ã•ã„ã€‚")
        st.stop()
    
    main()

