"""Streamlitãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³"""

import os
import time
import streamlit as st
from dotenv import load_dotenv
from typing import Dict, List

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

from constants import (
    DEFAULT_DATA_FOLDER, DEFAULT_K, DEFAULT_BM25_WEIGHT, DEFAULT_VECTOR_WEIGHT,
    DEFAULT_LLM_MODEL, MAX_CONVERSATION_HISTORY, DEFAULT_JUDGE_MODEL
)
from initialize import initialize_system
from retriever import search_with_scores, update_retriever_weights, update_retriever_k
from guardrails import apply_guardrails
from logger import log_query
from components import (
    render_citation, render_danger_banner, render_security_notice,
    render_chat_message
)
from error_handler import (
    DataFolderEmptyError, PDFReadError, IndexNotBuiltError, APIError,
    handle_data_folder_empty, handle_pdf_read_error, handle_index_not_built,
    handle_api_error, display_error_summary
)
from judge import (
    load_eval_dataset, run_evaluation_suite, save_evaluation_results,
    format_evaluation_summary
)

# ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
load_dotenv()

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="AIã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£é‹ç”¨æ‰‹é †æ›¸ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ",
    page_icon="ğŸ”’",
    layout="wide"
)

# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
if 'messages' not in st.session_state:
    st.session_state.messages = []

if 'vectorstore' not in st.session_state:
    st.session_state.vectorstore = None

if 'hybrid_retriever' not in st.session_state:
    st.session_state.hybrid_retriever = None

if 'chunks_metadata' not in st.session_state:
    st.session_state.chunks_metadata = {}

if 'data_folder' not in st.session_state:
    st.session_state.data_folder = DEFAULT_DATA_FOLDER

if 'k' not in st.session_state:
    st.session_state.k = DEFAULT_K

if 'bm25_weight' not in st.session_state:
    st.session_state.bm25_weight = DEFAULT_BM25_WEIGHT

if 'vector_weight' not in st.session_state:
    st.session_state.vector_weight = DEFAULT_VECTOR_WEIGHT

if 'index_last_built' not in st.session_state:
    st.session_state.index_last_built = None

if 'index_count' not in st.session_state:
    st.session_state.index_count = 0

if 'session_id' not in st.session_state:
    import uuid
    st.session_state.session_id = str(uuid.uuid4())

if 'eval_running' not in st.session_state:
    st.session_state.eval_running = False

if 'eval_results' not in st.session_state:
    st.session_state.eval_results = None


def handle_query(user_query: str, session_state: Dict) -> Dict:
    """è³ªå•å‡¦ç†ã®ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
    start_time = time.time()

    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æœªæ§‹ç¯‰ãƒã‚§ãƒƒã‚¯
    if session_state['hybrid_retriever'] is None:
        raise IndexNotBuiltError("ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒæ§‹ç¯‰ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")

    # Retrieverã®é‡ã¿ã¨kã‚’æ›´æ–°
    update_retriever_weights(
        session_state['hybrid_retriever'],
        session_state['bm25_weight'],
        session_state['vector_weight']
    )
    update_retriever_k(
        session_state['hybrid_retriever'],
        session_state['k']
    )

    # æ¤œç´¢å®Ÿè¡Œ
    try:
        search_results = search_with_scores(
            ensemble_retriever=session_state['hybrid_retriever'],
            query=user_query,
            k=session_state['k']
        )
    except Exception as e:
        raise APIError(f"æ¤œç´¢å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
    
    # ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«é©ç”¨ï¼ˆæ¤œç´¢å¾Œã€LLMå‘¼ã³å‡ºã—å‰ï¼‰
    guardrail_result = apply_guardrails(
        query=user_query,
        search_results=search_results,
        answer=None  # ã¾ã å›ç­”ã¯ç”Ÿæˆã—ã¦ã„ãªã„
    )
    
    # æ ¹æ‹ ä¸è¶³ã¾ãŸã¯æ›–æ˜§è³ªå•ã®å ´åˆã¯ã€ã“ã“ã§çµ‚äº†
    if not guardrail_result['should_respond']:
        processing_time = time.time() - start_time
        
        return {
            'answer': guardrail_result['answer'],
            'citations': guardrail_result['citations'],
            'flags': guardrail_result['flags'],
            'warning_reason': guardrail_result['warning_reason'],
            'top_score': guardrail_result['top_score'],
            'processing_time': processing_time,
            'token_usage': {'prompt_tokens': 0, 'completion_tokens': 0, 'total_tokens': 0},
            'cost': 0.0
        }
    
    # LLMãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
    context_text = "\n\n".join([
        f"ã€{i+1}ã€‘{result['text']}\nï¼ˆå‡ºå…¸: {result['file']} > {result['heading']}ï¼‰"
        for i, result in enumerate(search_results[:session_state['k']])
    ])
    
    # ä¼šè©±å±¥æ­´ã‚’å–å¾—ï¼ˆæœ€æ–°5å¾€å¾©ï¼‰
    conversation_history = session_state['messages'][-MAX_CONVERSATION_HISTORY * 2:]
    history_text = ""
    for msg in conversation_history:
        if msg['role'] == 'user':
            history_text += f"ãƒ¦ãƒ¼ã‚¶ãƒ¼: {msg['content']}\n"
        elif msg['role'] == 'assistant':
            history_text += f"ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ: {msg['content']}\n"
    
    prompt_template = ChatPromptTemplate.from_messages([
        ("system", """ã‚ãªãŸã¯SOCï¼ˆã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£é‹ç”¨ã‚»ãƒ³ã‚¿ãƒ¼ï¼‰ã®é‹ç”¨æ‰‹é †æ›¸ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
æä¾›ã•ã‚ŒãŸæ‰‹é †æ›¸ã®å†…å®¹ã«åŸºã¥ã„ã¦ã€æ­£ç¢ºã§åˆ†ã‹ã‚Šã‚„ã™ã„å›ç­”ã‚’ã—ã¦ãã ã•ã„ã€‚

é‡è¦ãªãƒ«ãƒ¼ãƒ«:
- æ‰‹é †æ›¸ã®å†…å®¹ã®ã¿ã‚’æ ¹æ‹ ã¨ã—ã¦å›ç­”ã—ã¦ãã ã•ã„
- æ¨æ¸¬ã‚„æ†¶æ¸¬ã¯é¿ã‘ã€æ‰‹é †æ›¸ã«è¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹æƒ…å ±ã®ã¿ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„
- å›ç­”ã®æœ€å¾Œã«ã€å‚è€ƒã«ã—ãŸæ‰‹é †æ›¸ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ˜è¨˜ã—ã¦ãã ã•ã„
- åˆ†ã‹ã‚‰ãªã„å ´åˆã¯ã€Œè©²å½“ã™ã‚‹æ‰‹é †ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€ã¨å›ç­”ã—ã¦ãã ã•ã„"""),
        ("human", """ä»¥ä¸‹ã®æ‰‹é †æ›¸ã®å†…å®¹ã‚’å‚è€ƒã«ã€è³ªå•ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚

ã€æ‰‹é †æ›¸ã®å†…å®¹ã€‘
{context}

ã€ä¼šè©±å±¥æ­´ã€‘
{history}

ã€è³ªå•ã€‘
{question}

ã€å›ç­”ã€‘""")
    ])
    
    # LLMåˆæœŸåŒ–
    llm = ChatOpenAI(
        model=DEFAULT_LLM_MODEL,
        temperature=0.0
    )
    
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå®Ÿè¡Œï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¯¾å¿œï¼‰
    prompt = prompt_template.format_messages(
        context=context_text,
        history=history_text,
        question=user_query
    )
    
    # APIå‘¼ã³å‡ºã—ï¼ˆãƒªãƒˆãƒ©ã‚¤å¯¾å¿œï¼‰
    max_retries = 3
    retry_count = 0
    response = None
    
    while retry_count < max_retries:
        try:
            response = llm.invoke(prompt)
            break
        except Exception as e:
            retry_count += 1
            should_retry, error_msg = handle_api_error(e, retry_count)
            if not should_retry:
                raise APIError(error_msg)
            if retry_count < max_retries:
                time.sleep(2 ** retry_count)  # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•
            else:
                raise APIError(f"APIå‘¼ã³å‡ºã—ãŒ{max_retries}å›å¤±æ•—ã—ã¾ã—ãŸ: {e}")
    
    answer = response.content.strip()
    
    # ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«é©ç”¨ï¼ˆå›ç­”ç”Ÿæˆå¾Œã€å±é™ºæ“ä½œæ¤œçŸ¥ï¼‰
    guardrail_result = apply_guardrails(
        query=user_query,
        search_results=search_results,
        answer=answer  # ç”Ÿæˆã—ãŸå›ç­”ã‚’ãƒã‚§ãƒƒã‚¯
    )
    
    # ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã¨ã‚³ã‚¹ãƒˆè¨ˆç®—
    from utils import calculate_cost
    
    # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã‚’å–å¾—
    if hasattr(response, 'response_metadata') and 'token_usage' in response.response_metadata:
        token_usage = response.response_metadata['token_usage']
    else:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æ¦‚ç®—
        estimated_prompt_tokens = len(context_text.split()) + len(user_query.split()) * 2
        estimated_completion_tokens = len(answer.split()) * 1.3
        token_usage = {
            'prompt_tokens': int(estimated_prompt_tokens),
            'completion_tokens': int(estimated_completion_tokens),
            'total_tokens': int(estimated_prompt_tokens + estimated_completion_tokens)
        }
    
    cost = calculate_cost(
        token_usage.get('prompt_tokens', 0),
        token_usage.get('completion_tokens', 0),
        DEFAULT_LLM_MODEL
    )
    
    processing_time = time.time() - start_time
    
    return {
        'answer': guardrail_result['answer'],
        'citations': guardrail_result['citations'],
        'flags': guardrail_result['flags'],
        'warning_reason': guardrail_result['warning_reason'],
        'top_score': guardrail_result['top_score'],
        'processing_time': processing_time,
        'token_usage': token_usage,
        'cost': cost
    }


def render_sidebar():
    """ã‚µã‚¤ãƒ‰ãƒãƒ¼æç”»"""
    with st.sidebar:
        st.title("âš™ï¸ è¨­å®š")
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹
        data_folder = st.text_input(
            "ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹",
            value=st.session_state.data_folder,
            help="æ‰‹é †æ›¸ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆPDF/Markdownï¼‰ã‚’æ ¼ç´ã—ã¦ã„ã‚‹ãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ‘ã‚¹"
        )
        st.session_state.data_folder = data_folder
        
        # kè¨­å®š
        k = st.number_input(
            "æ¤œç´¢çµæœæ•° (k)",
            min_value=1,
            max_value=20,
            value=st.session_state.k,
            help="æ¤œç´¢çµæœã¨ã—ã¦å–å¾—ã™ã‚‹ãƒãƒ£ãƒ³ã‚¯æ•°"
        )
        st.session_state.k = int(k)
        
        # é‡ã¿è¨­å®š
        st.subheader("æ¤œç´¢é‡ã¿")
        bm25_weight = st.slider(
            "BM25é‡ã¿",
            min_value=0.0,
            max_value=1.0,
            value=st.session_state.bm25_weight,
            step=0.1,
            help="BM25æ¤œç´¢ã®é‡ã¿ï¼ˆ0.0-1.0ï¼‰"
        )
        st.session_state.bm25_weight = bm25_weight
        st.session_state.vector_weight = 1.0 - bm25_weight
        
        st.write(f"ãƒ™ã‚¯ãƒˆãƒ«é‡ã¿: {st.session_state.vector_weight:.1f}")
        
        st.divider()
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å†æ§‹ç¯‰ãƒœã‚¿ãƒ³
        if st.button("ğŸ”„ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å†æ§‹ç¯‰", type="primary"):
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            try:
                status_text.text("ğŸ“‚ ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚©ãƒ«ãƒ€ã‚’ç¢ºèªä¸­...")
                progress_bar.progress(10)
                
                status_text.text("ğŸ“„ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã¿ä¸­...")
                progress_bar.progress(30)
                
                result = initialize_system(
                    st.session_state.data_folder,
                    bm25_weight=st.session_state.bm25_weight,
                    vector_weight=st.session_state.vector_weight,
                    k=st.session_state.k
                )
                
                status_text.text("ğŸ” ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°ä¸­...")
                progress_bar.progress(50)
                
                status_text.text("ğŸ’¾ ãƒ™ã‚¯ãƒˆãƒ«DBã‚’æ§‹ç¯‰ä¸­...")
                progress_bar.progress(70)
                
                status_text.text("ğŸ” BM25ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ§‹ç¯‰ä¸­...")
                progress_bar.progress(90)
                
                # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’æ›´æ–°
                st.session_state.vectorstore = result['vectorstore']
                st.session_state.hybrid_retriever = result['hybrid_retriever']
                st.session_state.chunks_metadata = result['chunks_metadata']
                st.session_state.index_count = result['index_count']
                st.session_state.index_last_built = result['index_last_built']
                
                progress_bar.progress(100)
                status_text.text("âœ… å®Œäº†")
                
                st.success(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰å®Œäº†: {result['index_count']}ä»¶ã®ãƒãƒ£ãƒ³ã‚¯ã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–ã—ã¾ã—ãŸ")
                
                # ã‚¨ãƒ©ãƒ¼ãŒã‚ã‚‹å ´åˆã¯è¡¨ç¤º
                if result.get('errors'):
                    display_error_summary(result['errors'])
                    
            except DataFolderEmptyError as e:
                progress_bar.empty()
                status_text.empty()
                handle_data_folder_empty(st.session_state.data_folder)
            except Exception as e:
                progress_bar.empty()
                status_text.empty()
                st.error(f"âŒ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                import traceback
                with st.expander("ã‚¨ãƒ©ãƒ¼è©³ç´°", expanded=False):
                    st.code(traceback.format_exc())
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹çŠ¶æ…‹è¡¨ç¤º
        st.divider()
        st.subheader("ğŸ“Š ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹çŠ¶æ…‹")
        if st.session_state.index_count > 0:
            st.write(f"**ãƒãƒ£ãƒ³ã‚¯æ•°**: {st.session_state.index_count}")
            if st.session_state.index_last_built:
                st.write(f"**æœ€çµ‚æ›´æ–°**: {st.session_state.index_last_built}")
        else:
            st.info("ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒæ§‹ç¯‰ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚å†æ§‹ç¯‰ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")

        # è©•ä¾¡å®Ÿè¡Œã‚»ã‚¯ã‚·ãƒ§ãƒ³
        st.divider()
        st.subheader("ğŸ§ª è©•ä¾¡å®Ÿè¡Œ")
        st.caption("LLM as a Judgeã§å›ç­”å“è³ªã‚’è©•ä¾¡ã—ã¾ã™ï¼ˆã‚³ã‚¹ãƒˆãŒã‹ã‹ã‚Šã¾ã™ï¼‰")

        if st.button("ğŸ“Š è©•ä¾¡ã‚’å®Ÿè¡Œ", type="secondary", disabled=st.session_state.hybrid_retriever is None):
            st.session_state.eval_running = True

        # è©•ä¾¡çµæœã®è¡¨ç¤º
        if st.session_state.eval_results is not None:
            summary = st.session_state.eval_results['summary']
            if summary['mvp_passed']:
                st.success(f"âœ… MVPåˆæ ¼ ({summary['average_score']:.1f}ç‚¹)")
            else:
                st.warning(f"âŒ MVPä¸åˆæ ¼ ({summary['average_score']:.1f}ç‚¹)")
            st.write(f"åˆæ ¼: {summary['passed_questions']}/{summary['total_questions']}å•")


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    
    # æ©Ÿå¯†æƒ…å ±æ³¨æ„è¡¨ç¤º
    render_security_notice()
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼
    render_sidebar()
    
    # ã‚¿ã‚¤ãƒˆãƒ«
    st.title("ğŸ”’ AIã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£é‹ç”¨æ‰‹é †æ›¸ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ")
    st.markdown("æ‰‹é †æ›¸ã‚’æ¤œç´¢ã—ã¦ã€è³ªå•ã«å›ç­”ã—ã¾ã™ã€‚")
    
    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒæ§‹ç¯‰ã•ã‚Œã¦ã„ãªã„å ´åˆã¯åˆæœŸåŒ–ã‚’ä¿ƒã™
    if st.session_state.hybrid_retriever is None:
        st.warning("âš ï¸ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒæ§‹ç¯‰ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰ã€Œã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å†æ§‹ç¯‰ã€ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")

        # è‡ªå‹•åˆæœŸåŒ–ã‚’è©¦ã¿ã‚‹
        if st.button("è‡ªå‹•åˆæœŸåŒ–ã‚’è©¦ã™"):
            with st.spinner("åˆæœŸåŒ–ä¸­..."):
                result = initialize_system(
                    st.session_state.data_folder,
                    bm25_weight=st.session_state.bm25_weight,
                    vector_weight=st.session_state.vector_weight,
                    k=st.session_state.k
                )
                if result['index_count'] > 0:
                    st.session_state.vectorstore = result['vectorstore']
                    st.session_state.hybrid_retriever = result['hybrid_retriever']
                    st.session_state.chunks_metadata = result['chunks_metadata']
                    st.session_state.index_count = result['index_count']
                    st.session_state.index_last_built = result['index_last_built']
                    st.success(f"åˆæœŸåŒ–å®Œäº†: {result['index_count']}ä»¶ã®ãƒãƒ£ãƒ³ã‚¯ã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–ã—ã¾ã—ãŸ")
                    st.rerun()
                else:
                    st.error("ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚©ãƒ«ãƒ€ã«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return

    # è©•ä¾¡å®Ÿè¡Œå‡¦ç†
    if st.session_state.eval_running:
        st.session_state.eval_running = False  # ãƒ•ãƒ©ã‚°ã‚’ãƒªã‚»ãƒƒãƒˆ

        with st.spinner("è©•ä¾¡ã‚’å®Ÿè¡Œä¸­... ã“ã‚Œã«ã¯æ•°åˆ†ã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™"):
            try:
                # è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿
                eval_dataset = load_eval_dataset()

                # è©•ä¾¡å®Ÿè¡Œ
                eval_results = run_evaluation_suite(
                    eval_dataset=eval_dataset,
                    answer_generator_func=handle_query,
                    session_state=st.session_state,
                    model=DEFAULT_JUDGE_MODEL
                )

                # çµæœã‚’ä¿å­˜
                saved_path = save_evaluation_results(eval_results)

                # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«ä¿å­˜
                st.session_state.eval_results = eval_results

                # æˆåŠŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
                st.success(f"è©•ä¾¡å®Œäº†ï¼çµæœã‚’ {saved_path} ã«ä¿å­˜ã—ã¾ã—ãŸ")

                # ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º
                st.markdown(format_evaluation_summary(eval_results['summary']))

                # è©³ç´°çµæœã‚’è¡¨ç¤º
                with st.expander("ğŸ“‹ è©³ç´°çµæœã‚’è¦‹ã‚‹"):
                    for result in eval_results['results']:
                        q_id = result['question_id']
                        category = result['category']
                        question = result['question']
                        eval_data = result['evaluation']

                        # å•é¡Œã”ã¨ã®çµæœ
                        passed_icon = "âœ…" if eval_data['passed'] else "âŒ"
                        st.markdown(f"### {passed_icon} å•é¡Œ {q_id} ({category}) - {eval_data['average_score']:.1f}ç‚¹")
                        st.markdown(f"**è³ªå•**: {question}")

                        # ã‚¹ã‚³ã‚¢è¡¨ç¤º
                        st.markdown("**ã‚¹ã‚³ã‚¢**:")
                        cols = st.columns(3)
                        scores = eval_data['scores']
                        for i, (criteria, score) in enumerate(scores.items()):
                            col_idx = i % 3
                            cols[col_idx].metric(criteria, f"{score}ç‚¹")

                        # ç·åˆã‚³ãƒ¡ãƒ³ãƒˆ
                        st.markdown(f"**ç·åˆã‚³ãƒ¡ãƒ³ãƒˆ**: {eval_data['overall_comment']}")
                        st.divider()

                st.rerun()

            except Exception as e:
                st.error(f"è©•ä¾¡ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                import traceback
                st.code(traceback.format_exc())

    # ä¼šè©±å±¥æ­´è¡¨ç¤º
    for message in st.session_state.messages:
        render_chat_message(
            role=message['role'],
            content=message['content'],
            citations=message.get('citations', [])
        )
        
        # å±é™ºæ“ä½œè­¦å‘ŠãƒãƒŠãƒ¼ï¼ˆå„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã”ã¨ã«è¡¨ç¤ºï¼‰
        if message.get('flags', {}).get('dangerous_operation', False):
            render_danger_banner()
    
    # è³ªå•å…¥åŠ›
    if prompt := st.chat_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„"):
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
        st.session_state.messages.append({
            'role': 'user',
            'content': prompt,
            'citations': []
        })
        
        # ä¼šè©±å±¥æ­´ã‚’åˆ¶é™ï¼ˆç›´è¿‘5å¾€å¾© = 10ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼‰
        if len(st.session_state.messages) > MAX_CONVERSATION_HISTORY * 2:
            st.session_state.messages = st.session_state.messages[-MAX_CONVERSATION_HISTORY * 2:]
        
        # è³ªå•å‡¦ç†
        try:
            with st.spinner("å›ç­”ã‚’ç”Ÿæˆä¸­..."):
                result = handle_query(prompt, st.session_state)
                
                # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ ï¼ˆè­¦å‘Šã¯æ—¢ã«apply_guardrailsã§è¿½åŠ æ¸ˆã¿ï¼‰
                st.session_state.messages.append({
                    'role': 'assistant',
                    'content': result['answer'],
                    'citations': result['citations'],
                    'flags': result['flags'],
                    'warning_reason': result.get('warning_reason')
                })
                
                # ãƒ­ã‚°è¨˜éŒ²
                log_query(
                    query=prompt,
                    search_results=result['citations'],
                    answer=result['answer'],
                    processing_time=result['processing_time'],
                    token_usage=result['token_usage'],
                    cost=result['cost'],
                    search_config={
                        'k': st.session_state.k,
                        'bm25_weight': st.session_state.bm25_weight,
                        'vector_weight': st.session_state.vector_weight
                    },
                    flags=result['flags'],
                    warning_reason=result.get('warning_reason'),
                    top_score=result.get('top_score', 0.0),
                    session_id=st.session_state.session_id
                )
                
                # ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·è­¦å‘Šï¼ˆ5ç§’è¶…éï¼‰
                if result['processing_time'] > 5.0:
                    st.warning(f"âš ï¸ å‡¦ç†æ™‚é–“ãŒ5ç§’ã‚’è¶…éã—ã¾ã—ãŸï¼ˆ{result['processing_time']:.1f}ç§’ï¼‰")
        
        except IndexNotBuiltError:
            handle_index_not_built()
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å‰Šé™¤ï¼ˆã‚¨ãƒ©ãƒ¼æ™‚ã¯è¿½åŠ ã—ãªã„ï¼‰
            st.session_state.messages.pop()
        except APIError as e:
            st.error(f"âŒ {e}")
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å‰Šé™¤
            st.session_state.messages.pop()
        except Exception as e:
            st.error(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            import traceback
            with st.expander("ã‚¨ãƒ©ãƒ¼è©³ç´°", expanded=False):
                st.code(traceback.format_exc())
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å‰Šé™¤
            st.session_state.messages.pop()
        
        # ç”»é¢ã‚’æ›´æ–°
        st.rerun()


if __name__ == "__main__":
    # OpenAI APIã‚­ãƒ¼ã®ç¢ºèª
    if not os.getenv("OPENAI_API_KEY"):
        st.error("âš ï¸ OPENAI_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚.envãƒ•ã‚¡ã‚¤ãƒ«ã¾ãŸã¯ç’°å¢ƒå¤‰æ•°ã§è¨­å®šã—ã¦ãã ã•ã„ã€‚")
        st.stop()
    
    main()

